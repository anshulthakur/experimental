{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e384350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131b696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import basic libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import settings\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from django_pandas.io import read_frame\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "\n",
    "from matplotlib.dates import date2num\n",
    "\n",
    "# imports\n",
    "import pandas_datareader.data as pdr\n",
    "import datetime\n",
    "import talib\n",
    "from talib.abstract import *\n",
    "from talib import MA_Type\n",
    "\n",
    "# format price data\n",
    "pd.options.display.float_format = '{:0.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d78f1",
   "metadata": {},
   "source": [
    "## Pattern recognition on the OHLC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04547ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prepare to load stock data as pandas dataframe from source. In this case, prepare django\n",
    "import django\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'rest.settings')\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "django.setup()\n",
    "\n",
    "from stocks.models import Listing, Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717729b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "def get_stock_listing(stock, duration=None, last_date = datetime.date.today(), studies=None, resample=False, monthly=False):\n",
    "    requested_duration = duration\n",
    "    if duration == None and studies==None:\n",
    "        duration = 365\n",
    "        requested_duration = duration\n",
    "    elif duration == -1:\n",
    "        duration = 3650\n",
    "    elif studies is not None:\n",
    "        #studies={'daily': ['rsi', 'ema20', 'ema10', 'sma200'],\n",
    "        #                   'weekly':['rsi', 'ema20', 'ema10'],\n",
    "        #                   'monthly': ['rsi']}\n",
    "        if studies.get('monthly', None) is not None:\n",
    "            if 'rsi' in studies.get('monthly'):\n",
    "                duration = 500 #Need at least 14 months data for RSI\n",
    "        elif studies.get('weekly', None) is not None:\n",
    "            if 'rsi' in studies.get('weekly'):\n",
    "                duration = 14*7 if duration is None else max(duration, 14*7)#Need at least 14 weeks data for RSI\n",
    "            if 'ema20' in studies.get('weekly'):\n",
    "                duration = 21*7 if duration is None else max(duration, 14*7)#Need at least 20 weeks data for EMA\n",
    "            if 'ema10' in studies.get('weekly'):\n",
    "                duration = 11*7 if duration is None else max(duration, 11*7)#Need at least 20 weeks data for EMA\n",
    "        elif studies.get('daily', None) is not None:\n",
    "            if 'rsi' in studies.get('daily'):\n",
    "                duration = 14 if duration is None else max(duration, 14)#Need at least 14 days data for RSI\n",
    "            if 'ema20' in studies.get('daily'):\n",
    "                duration = 21 if duration is None else max(duration, 21)#Need at least 20 days data for EMA\n",
    "            if 'ema10' in studies.get('daily'):\n",
    "                duration = 11 if duration is None else max(duration, 11)#Need at least 10 days data for EMA\n",
    "            if 'sma200' in studies.get('daily'):\n",
    "                duration = 291 if duration is None else max(duration, 291)#Need at least 291 days data for SMA\n",
    "                \n",
    "    #print(duration)\n",
    "    first_date = last_date - datetime.timedelta(days=duration)\n",
    "    listing = Listing.objects.filter(stock=stock, date__range=(first_date, last_date))\n",
    "    df = read_frame(listing, index_col='date')\n",
    "    for column in df.columns:\n",
    "        if column != 'stock':\n",
    "           df[column] = pd.to_numeric(df[column])\n",
    "    df = df.sort_index()\n",
    "    df = df.reindex(columns = ['opening', 'high', 'low', 'closing', 'traded', 'deliverable', 'trades'])\n",
    "    df.rename(columns={\"opening\": \"open\", \n",
    "                       \"high\": \"high\", \n",
    "                       \"low\": \"low\", \n",
    "                       \"closing\":\"close\", \n",
    "                       \"traded\":\"volume\", \n",
    "                       'deliverable':'delivery',\n",
    "                       'trades':'trades'}, inplace=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    #Delete duplicate columns\n",
    "    #df = df.loc[:,~df.columns.duplicated()]\n",
    "    df.drop_duplicates(inplace = True)\n",
    "    df.dropna(inplace=True)\n",
    "    if resample:\n",
    "        #Resample weekly\n",
    "        logic = {'open'  : 'first',\n",
    "                 'high'  : 'max',\n",
    "                 'low'   : 'min',\n",
    "                 'close' : 'last',\n",
    "                 'volume': 'sum',\n",
    "                 'delivery': 'sum',\n",
    "                 'trades': 'sum'}\n",
    "        #Resample on weekly levels\n",
    "        if monthly:\n",
    "            df = df.resample('M').apply(logic)\n",
    "        else:\n",
    "            df = df.resample('W').apply(logic)\n",
    "            df.index -= to_offset(\"6D\")\n",
    "        \n",
    "    #Add the studies inplace \n",
    "    if studies is not None and studies.get('daily', None) is not None and len(df)>0:\n",
    "        if 'rsi' in studies.get('daily'):\n",
    "            df['rsi'] = talib.RSI(df['close'], 14)\n",
    "        if 'ema20' in studies.get('daily'):\n",
    "            df['ema20'] = talib.EMA(df['close'], 20)\n",
    "        if 'ema10' in studies.get('daily'):\n",
    "            df['ema10'] = talib.EMA(df['close'], 10)\n",
    "        if 'sma200' in studies.get('daily'):\n",
    "            df['sma200'] = talib.SMA(df['close'], 200)\n",
    "    if studies is not None and studies.get('weekly', None) is not None and len(df)>0:\n",
    "        if 'rsi' in studies.get('weekly'):\n",
    "            df['rsi'] = talib.RSI(df['close'], 14)\n",
    "        if 'ema20' in studies.get('daily'):\n",
    "            df['ema20'] = talib.EMA(df['close'], 20)\n",
    "        if 'ema10' in studies.get('daily'):\n",
    "            df['ema10'] = talib.EMA(df['close'], 10)\n",
    "    \n",
    "            \n",
    "\n",
    "    #print(df.tail())\n",
    "    #Optionally, filter out by date range\n",
    "    #start_date = '2020-01-01'\n",
    "    #end_date = '2021-12-31'\n",
    "    #df = df.loc[start_date:end_date]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32e714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/CanerIrfanoglu/medium/blob/master/candle_stick_recognition/candle_rankings.py\n",
    "#https://medium.com/analytics-vidhya/recognizing-over-50-candlestick-patterns-with-python-4f02a1822cb5\n",
    "candle_rankings = {\n",
    "        \"CDL3LINESTRIKE_Bull\": 1,\n",
    "        \"CDL3LINESTRIKE_Bear\": 2,\n",
    "        \"CDL3BLACKCROWS_Bull\": 3,\n",
    "        \"CDL3BLACKCROWS_Bear\": 3,\n",
    "        \"CDLEVENINGSTAR_Bull\": 4,\n",
    "        \"CDLEVENINGSTAR_Bear\": 4,\n",
    "        \"CDLTASUKIGAP_Bull\": 5,\n",
    "        \"CDLTASUKIGAP_Bear\": 5,\n",
    "        \"CDLINVERTEDHAMMER_Bull\": 6,\n",
    "        \"CDLINVERTEDHAMMER_Bear\": 6,\n",
    "        \"CDLMATCHINGLOW_Bull\": 7,\n",
    "        \"CDLMATCHINGLOW_Bear\": 7,\n",
    "        \"CDLABANDONEDBABY_Bull\": 8,\n",
    "        \"CDLABANDONEDBABY_Bear\": 8,\n",
    "        \"CDLBREAKAWAY_Bull\": 10,\n",
    "        \"CDLBREAKAWAY_Bear\": 10,\n",
    "        \"CDLMORNINGSTAR_Bull\": 12,\n",
    "        \"CDLMORNINGSTAR_Bear\": 12,\n",
    "        \"CDLPIERCING_Bull\": 13,\n",
    "        \"CDLPIERCING_Bear\": 13,\n",
    "        \"CDLSTICKSANDWICH_Bull\": 14,\n",
    "        \"CDLSTICKSANDWICH_Bear\": 14,\n",
    "        \"CDLTHRUSTING_Bull\": 15,\n",
    "        \"CDLTHRUSTING_Bear\": 15,\n",
    "        \"CDLINNECK_Bull\": 17,\n",
    "        \"CDLINNECK_Bear\": 17,\n",
    "        \"CDL3INSIDE_Bull\": 20,\n",
    "        \"CDL3INSIDE_Bear\": 56,\n",
    "        \"CDLHOMINGPIGEON_Bull\": 21,\n",
    "        \"CDLHOMINGPIGEON_Bear\": 21,\n",
    "        \"CDLDARKCLOUDCOVER_Bull\": 22,\n",
    "        \"CDLDARKCLOUDCOVER_Bear\": 22,\n",
    "        \"CDLIDENTICAL3CROWS_Bull\": 24,\n",
    "        \"CDLIDENTICAL3CROWS_Bear\": 24,\n",
    "        \"CDLMORNINGDOJISTAR_Bull\": 25,\n",
    "        \"CDLMORNINGDOJISTAR_Bear\": 25,\n",
    "        \"CDLXSIDEGAP3METHODS_Bull\": 27,\n",
    "        \"CDLXSIDEGAP3METHODS_Bear\": 26,\n",
    "        \"CDLTRISTAR_Bull\": 28,\n",
    "        \"CDLTRISTAR_Bear\": 76,\n",
    "        \"CDLGAPSIDESIDEWHITE_Bull\": 46,\n",
    "        \"CDLGAPSIDESIDEWHITE_Bear\": 29,\n",
    "        \"CDLEVENINGDOJISTAR_Bull\": 30,\n",
    "        \"CDLEVENINGDOJISTAR_Bear\": 30,\n",
    "        \"CDL3WHITESOLDIERS_Bull\": 32,\n",
    "        \"CDL3WHITESOLDIERS_Bear\": 32,\n",
    "        \"CDLONNECK_Bull\": 33,\n",
    "        \"CDLONNECK_Bear\": 33,\n",
    "        \"CDL3OUTSIDE_Bull\": 34,\n",
    "        \"CDL3OUTSIDE_Bear\": 39,\n",
    "        \"CDLRICKSHAWMAN_Bull\": 35,\n",
    "        \"CDLRICKSHAWMAN_Bear\": 35,\n",
    "        \"CDLSEPARATINGLINES_Bull\": 36,\n",
    "        \"CDLSEPARATINGLINES_Bear\": 40,\n",
    "        \"CDLLONGLEGGEDDOJI_Bull\": 37,\n",
    "        \"CDLLONGLEGGEDDOJI_Bear\": 37,\n",
    "        \"CDLHARAMI_Bull\": 38,\n",
    "        \"CDLHARAMI_Bear\": 72,\n",
    "        \"CDLLADDERBOTTOM_Bull\": 41,\n",
    "        \"CDLLADDERBOTTOM_Bear\": 41,\n",
    "        \"CDLCLOSINGMARUBOZU_Bull\": 70,\n",
    "        \"CDLCLOSINGMARUBOZU_Bear\": 43,\n",
    "        \"CDLTAKURI_Bull\": 47,\n",
    "        \"CDLTAKURI_Bear\": 47,\n",
    "        \"CDLDOJISTAR_Bull\": 49,\n",
    "        \"CDLDOJISTAR_Bear\": 51,\n",
    "        \"CDLHARAMICROSS_Bull\": 50,\n",
    "        \"CDLHARAMICROSS_Bear\": 80,\n",
    "        \"CDLADVANCEBLOCK_Bull\": 54,\n",
    "        \"CDLADVANCEBLOCK_Bear\": 54,\n",
    "        \"CDLSHOOTINGSTAR_Bull\": 55,\n",
    "        \"CDLSHOOTINGSTAR_Bear\": 55,\n",
    "        \"CDLMARUBOZU_Bull\": 71,\n",
    "        \"CDLMARUBOZU_Bear\": 57,\n",
    "        \"CDLUNIQUE3RIVER_Bull\": 60,\n",
    "        \"CDLUNIQUE3RIVER_Bear\": 60,\n",
    "        \"CDL2CROWS_Bull\": 61,\n",
    "        \"CDL2CROWS_Bear\": 61,\n",
    "        \"CDLBELTHOLD_Bull\": 62,\n",
    "        \"CDLBELTHOLD_Bear\": 63,\n",
    "        \"CDLHAMMER_Bull\": 65,\n",
    "        \"CDLHAMMER_Bear\": 65,\n",
    "        \"CDLHIGHWAVE_Bull\": 67,\n",
    "        \"CDLHIGHWAVE_Bear\": 67,\n",
    "        \"CDLSPINNINGTOP_Bull\": 69,\n",
    "        \"CDLSPINNINGTOP_Bear\": 73,\n",
    "        \"CDLUPSIDEGAP2CROWS_Bull\": 74,\n",
    "        \"CDLUPSIDEGAP2CROWS_Bear\": 74,\n",
    "        \"CDLGRAVESTONEDOJI_Bull\": 77,\n",
    "        \"CDLGRAVESTONEDOJI_Bear\": 77,\n",
    "        \"CDLHIKKAKEMOD_Bull\": 82,\n",
    "        \"CDLHIKKAKEMOD_Bear\": 81,\n",
    "        \"CDLHIKKAKE_Bull\": 85,\n",
    "        \"CDLHIKKAKE_Bear\": 83,\n",
    "        \"CDLENGULFING_Bull\": 84,\n",
    "        \"CDLENGULFING_Bear\": 91,\n",
    "        \"CDLMATHOLD_Bull\": 86,\n",
    "        \"CDLMATHOLD_Bear\": 86,\n",
    "        \"CDLHANGINGMAN_Bull\": 87,\n",
    "        \"CDLHANGINGMAN_Bear\": 87,\n",
    "        \"CDLRISEFALL3METHODS_Bull\": 94,\n",
    "        \"CDLRISEFALL3METHODS_Bear\": 89,\n",
    "        \"CDLKICKING_Bull\": 96,\n",
    "        \"CDLKICKING_Bear\": 102,\n",
    "        \"CDLDRAGONFLYDOJI_Bull\": 98,\n",
    "        \"CDLDRAGONFLYDOJI_Bear\": 98,\n",
    "        \"CDLCONCEALBABYSWALL_Bull\": 101,\n",
    "        \"CDLCONCEALBABYSWALL_Bear\": 101,\n",
    "        \"CDL3STARSINSOUTH_Bull\": 103,\n",
    "        \"CDL3STARSINSOUTH_Bear\": 103,\n",
    "        \"CDLDOJI_Bull\": 104,\n",
    "        \"CDLDOJI_Bear\": 104,\n",
    "        \"CDLLONGLINE_Bull\": 53,\n",
    "        \"CDLLONGLINE_Bear\": 53,\n",
    "        \"CDLSHORTLINE_Bull\": 85,\n",
    "        \"CDLSHORTLINE_Bear\": 66,\n",
    "        \"CDLSTALLEDPATTERN_Bull\": 93,\n",
    "        \"CDLSTALLEDPATTERN_Bear\": 93,\n",
    "        \"CDLKICKINGBYLENGTH\": 96,\n",
    "        \"CDLKICKINGBYLENGTH_Bear\": 102\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe4af8",
   "metadata": {},
   "source": [
    "## How does one determine the efficiency of an indicator/signal?\n",
    "\n",
    "Currently, it seems hard that I could use the patterns alone in determining the trades (without understanding the patterns).\n",
    "So, what I can do is:\n",
    "\n",
    "1. Let the code run on daily basis and generate signals in the time frame (if daily, then for a month's data etc). \n",
    "   Print a summary of signals generated on the day, and manually evaluate them. (Lesser time to code, so we'll do that.)\n",
    "2. Try to also estimate the accuracy/reliability of the candlestick myself rather than relying on someone else's \n",
    "   blackbox evaluation. The question is, how to deduce the efficacy (what time frame)? One stab at the answer would be, \n",
    "   understand each indicator and have its custom window determined empirically or via text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5b55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_signal(df, signal_val, signal_frequency, success_frequency, tolerance=2):\n",
    "    '''\n",
    "    For the signal value passed, determine if price moves in the direction of signal \n",
    "    or not within the tolerance period and update the probability of success metric (frequentist)\n",
    "    '''\n",
    "    occurances = df[df[signal_val]!=0]\n",
    "    #print(occurances.tail())\n",
    "    idxs = []\n",
    "    for idx, occurance in occurances.iterrows():\n",
    "        #print(df.index.get_loc(idx))\n",
    "        idxs.append(df.index.get_loc(idx))\n",
    "\n",
    "    for idx in idxs:\n",
    "        #print(idx)\n",
    "        if df.iloc[idx][signal_val] > 0:\n",
    "            #Bullish\n",
    "            signal_frequency['bullish'][signal_val] += 1\n",
    "            if idx+tolerance <= len(df)-1 and df.iloc[idx+tolerance]['close']> df.iloc[idx]['close']:\n",
    "                #Pattern is success\n",
    "                success_frequency['bullish'][signal_val] +=1\n",
    "        elif df.iloc[idx][signal_val] < 0:\n",
    "            #Bearish\n",
    "            signal_frequency['bearish'][signal_val] += 1\n",
    "            if idx+tolerance <= len(df)-1 and df.iloc[idx+tolerance]['close']< df.iloc[idx]['close']:\n",
    "                #Pattern is success\n",
    "                success_frequency['bearish'][signal_val] +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e73f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For all stocks, for all days, run the above function once to determine the score of each indicator\n",
    "# create columns for each pattern\n",
    "candle_names = talib.get_function_groups()['Pattern Recognition']\n",
    "\n",
    "signal_frequency = {'bullish': {}, 'bearish':{}}\n",
    "success_frequency = {'bullish': {}, 'bearish':{}}\n",
    "\n",
    "for candle in candle_names:\n",
    "    signal_frequency['bullish'][candle] = 0\n",
    "    success_frequency['bullish'][candle] = 0\n",
    "    signal_frequency['bearish'][candle] = 0\n",
    "    success_frequency['bearish'][candle] = 0\n",
    "    \n",
    "#Asian paints\n",
    "#stock = Stock.objects.get(sid='ASIANPAINT')\n",
    "#listing = get_stock_listing(stock, duration=-1, last_date = datetime.date.today())\n",
    "#for candle in candle_names:\n",
    "#    listing[candle] = getattr(talib, candle)(listing['open'], listing['high'], listing['low'], listing['close'])\n",
    "#    evaluate_signal(listing, candle, signal_frequency, success_frequency)\n",
    "    \n",
    "for stock in Stock.objects.all():\n",
    "    #listing = get_stock_listing(stock, duration=-1, last_date = datetime.date.today(), resample=False)\n",
    "    listing = get_stock_listing(stock, duration=-1, last_date = datetime.date.today(), resample=False, monthly=False) #For weekly/monthly charts\n",
    "    if len(listing)==0:\n",
    "        continue\n",
    "    for candle in candle_names:\n",
    "        listing[candle] = getattr(talib, candle)(listing['open'], listing['high'], listing['low'], listing['close'])\n",
    "        evaluate_signal(listing, candle, signal_frequency, success_frequency, tolerance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635147a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Bullish')\n",
    "for key in success_frequency['bullish']:\n",
    "    if signal_frequency['bullish'][key] !=0:\n",
    "        print(f\"{key}: {success_frequency['bullish'][key]/signal_frequency['bullish'][key]}\")\n",
    "    else:\n",
    "        print(f'{key}: NA')\n",
    "print('Bearish')\n",
    "for key in success_frequency['bearish']:\n",
    "    if signal_frequency['bearish'][key] !=0:\n",
    "        print(f\"{key}: {success_frequency['bearish'][key]/signal_frequency['bearish'][key]}\")\n",
    "    else:\n",
    "        print(f'{key}: NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae98c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_fractals(df):\n",
    "    df['fractal'] = 0\n",
    "    \n",
    "    for i in range(2,df.shape[0]-2):\n",
    "        if df['low'][i] < df['low'][i-1]  and df['low'][i] < df['low'][i+1] and df['low'][i+1] < df['low'][i+2] and df['low'][i-1] < df['low'][i-2]:\n",
    "            #df['fractal'][i] = 1.0\n",
    "            df.iloc[i, df.columns.get_loc('fractal')] = 1.0\n",
    "        elif df['high'][i] > df['high'][i-1]  and df['high'][i] > df['high'][i+1] and df['high'][i+1] > df['high'][i+2] and df['high'][i-1] > df['high'][i-2]:\n",
    "            #df['fractal'][i] = -1.0\n",
    "            df.iat[i, df.columns.get_loc('fractal')] = -1.0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f68241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test if it detects fractals properly\n",
    "data = {'high': [1, 2, 3, 2, 1, 2, 3], 'low': [1, 2, 3, 2, 1, 2, 3]}  \n",
    "\n",
    "testdf = pd.DataFrame(data)\n",
    "\n",
    "testdf = detect_fractals(testdf)\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c64e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compute efficacy of fractals patterns as well\n",
    "\n",
    "signal_frequency = {'bullish': {}, 'bearish':{}}\n",
    "success_frequency = {'bullish': {}, 'bearish':{}}\n",
    "\n",
    "candle_names = ['fractal']\n",
    "for candle in candle_names:\n",
    "    signal_frequency['bullish'][candle] = 0\n",
    "    success_frequency['bullish'][candle] = 0\n",
    "    signal_frequency['bearish'][candle] = 0\n",
    "    success_frequency['bearish'][candle] = 0\n",
    "    \n",
    "    \n",
    "for stock in Stock.objects.all():\n",
    "    #listing = get_stock_listing(stock, duration=-1, last_date = datetime.date.today(), resample=False)\n",
    "    listing = get_stock_listing(stock, duration=-1, last_date = datetime.date.today(), resample=False, monthly=False) #For weekly/monthly charts\n",
    "    if len(listing)==0:\n",
    "        continue\n",
    "    for candle in candle_names:\n",
    "        df = detect_fractals(listing)\n",
    "        evaluate_signal(listing, candle, signal_frequency, success_frequency, tolerance=3)\n",
    "        \n",
    "print('Bullish')\n",
    "for key in success_frequency['bullish']:\n",
    "    if signal_frequency['bullish'][key] !=0:\n",
    "        print(f\"{key}: {success_frequency['bullish'][key]/signal_frequency['bullish'][key]}\")\n",
    "    else:\n",
    "        print(f'{key}: NA')\n",
    "print('Bearish')\n",
    "for key in success_frequency['bearish']:\n",
    "    if signal_frequency['bearish'][key] !=0:\n",
    "        print(f\"{key}: {success_frequency['bearish'][key]/signal_frequency['bearish'][key]}\")\n",
    "    else:\n",
    "        print(f'{key}: NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5b14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(signal_frequency['bullish']['fractal'])\n",
    "print(success_frequency['bullish']['fractal'])\n",
    "\n",
    "print(signal_frequency['bearish']['fractal'])\n",
    "print(success_frequency['bearish']['fractal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133996d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_trend(df):\n",
    "    '''\n",
    "    A stock is in some sort of trend on various time scales. \n",
    "    The hypothesis is that some chart patterns have a greater reliability in a particular trend.\n",
    "    So, we want to combine trend and other signals to create higher probability of profit.\n",
    "    \n",
    "    We want to test out this hypothesis.\n",
    "    '''\n",
    "    pass\n",
    "    \n",
    "def get_volume_signals(df):\n",
    "    '''\n",
    "    Look at the trading volume and delivery volumes\n",
    "    1. Check if unusually large amount of delivery volume is there (not percentages)\n",
    "    2. Check if unusually large trading volume is there\n",
    "    '''\n",
    "    signals = {'volume_shoot_up': None,\n",
    "               'volume_contract': None,\n",
    "               'delivery_shoot_up': None,\n",
    "               'delivery_contract': None,\n",
    "               'delivery_ptage_shoot_up': None,\n",
    "               'delivery_ptage_contract': None,\n",
    "               'volume_per_trade_shoot_up': None,\n",
    "               'volume_per_trade_contract': None,}\n",
    "    period = 20 #20 day mean\n",
    "    std = df['volume'].ewm(span=period).std()\n",
    "    \n",
    "    #std = df.iloc[-21:-1]['volume'].std()\n",
    "    vol_ema = talib.EMA(df['volume'], period)\n",
    "\n",
    "    if np.isnan(vol_ema.iloc[-2])==False and  std.iloc[-2] != 0 and df.iloc[-1]['volume'] >= (vol_ema.iloc[-2]+ std.iloc[-2]):\n",
    "        signals['volume_shoot_up'] = (df.iloc[-1]['volume'] - vol_ema.iloc[-2])/std.iloc[-2]\n",
    "    elif np.isnan(vol_ema.iloc[-2])==False and std.iloc[-2] != 0 and df.iloc[-1]['volume'] <= (vol_ema.iloc[-2]- std.iloc[-2]):\n",
    "        signals['volume_contract'] = (vol_ema.iloc[-2] - df.iloc[-1]['volume'])/std.iloc[-2]\n",
    "    #else:\n",
    "    #    print(f\"Volume: {df.iloc[-1]['volume']}  EMA: {vol_ema.iloc[-2]} STD: {std}\")\n",
    "        \n",
    "    #std = df[-21:-1]['delivery'].std()\n",
    "    std = df['delivery'].ewm(span=period).std()\n",
    "    del_ema = talib.EMA(df['delivery'], period)\n",
    "    if np.isnan(del_ema.iloc[-2])==False and std.iloc[-2] != 0 and df.iloc[-1]['delivery'] >= (del_ema.iloc[-2]+ std.iloc[-2]):\n",
    "        signals['delivery_shoot_up'] = (df.iloc[-1]['delivery'] - del_ema.iloc[-2])/std.iloc[-2]\n",
    "    elif np.isnan(del_ema.iloc[-2])==False and std.iloc[-2] != 0 and df.iloc[-1]['delivery'] <= (del_ema.iloc[-2]- std.iloc[-2]):\n",
    "        signals['delivery_contract'] = (del_ema.iloc[-2] - df.iloc[-1]['delivery'])/std.iloc[-2]\n",
    "    #else:\n",
    "    #    print(f\"Delivery: {df.iloc[-1]['delivery']}  EMA: {del_ema.iloc[-2]} STD: {std}\")\n",
    "    \n",
    "    dl_ptage = df['delivery']/df['volume']\n",
    "    dl_ema = talib.SMA(dl_ptage, period)\n",
    "    #std = dl_ptage[-21:-1].std()\n",
    "    std = dl_ptage.ewm(span=period).std()\n",
    "    if np.isnan(dl_ema.iloc[-2])==False and std.iloc[-2] != 0 and dl_ptage.iloc[-1] >= (dl_ema.iloc[-2]+ std.iloc[-2]):\n",
    "        signals['delivery_ptage_shoot_up'] = (dl_ptage.iloc[-1] - dl_ema.iloc[-2])/std.iloc[-2]\n",
    "    elif np.isnan(dl_ema.iloc[-2])==False and std.iloc[-2] != 0 and dl_ptage.iloc[-1] <= (dl_ema.iloc[-2]- std.iloc[-2]):\n",
    "        signals['delivery_ptage_contract'] = (dl_ema.iloc[-2] - dl_ptage.iloc[-1])/std.iloc[-2]\n",
    "    #else:\n",
    "    #    print(f\"Delivery %: {dl_ptage.iloc[-1]}  EMA: {dl_ema.iloc[-2]} STD: {std}\")\n",
    "    \n",
    "    vpt_ptage = df['volume']/df['trades'] #Volume per trade\n",
    "    dl_ema = talib.SMA(vpt_ptage, period)\n",
    "    #std = vpt_ptage[-21:-1].std()\n",
    "    std = vpt_ptage.ewm(span=period).std()\n",
    "    if np.isnan(dl_ema.iloc[-2])==False and std.iloc[-2] != 0 and vpt_ptage.iloc[-1] >= (dl_ema.iloc[-2]+ std.iloc[-2]):\n",
    "        signals['volume_per_trade_shoot_up'] = (vpt_ptage.iloc[-1] - dl_ema.iloc[-2])/std.iloc[-2]\n",
    "    elif np.isnan(dl_ema.iloc[-2])==False and std.iloc[-2] != 0 and vpt_ptage.iloc[-1] <= (dl_ema.iloc[-2]- std.iloc[-2]):\n",
    "        signals['volume_per_trade_contract'] = (dl_ema.iloc[-2] - vpt_ptage.iloc[-1])/std.iloc[-2]\n",
    "    #else:\n",
    "    #    print(f\"Volume Per trade %: {vpt_ptage.iloc[-1]}  EMA: {dl_ema.iloc[-2]} STD: {std}\")\n",
    "     \n",
    "    \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593f400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_rrg(df_reference, df_target):\n",
    "    '''\n",
    "    Compute the relative rotation graph (RRG) values of df_target w.r.t. df_reference\n",
    "    \n",
    "    The idea is to find the sectors which are likely to go up, and then find the leaders in \n",
    "    that sector to improve the probability of profit.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38ca40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_last_day_patterns(df):\n",
    "    candle_names = talib.get_function_groups()['Pattern Recognition']\n",
    "    patterns = {'bullish': [], 'bearish': []}\n",
    "    for candle in candle_names:\n",
    "        df[candle] = getattr(talib, candle)(df['open'], df['high'], df['low'], df['close'])\n",
    "        if df.iloc[-1][candle]>0:\n",
    "            patterns['bullish'].append(candle)\n",
    "        elif df.iloc[-1][candle]<0:\n",
    "            patterns['bearish'].append(candle)\n",
    "    df = detect_fractals(df)\n",
    "    if df.iloc[-3]['fractal']>0:\n",
    "        patterns['bullish'].append('fractal')\n",
    "    elif df.iloc[-3]['fractal']<0:\n",
    "        patterns['bearish'].append('fractal')\n",
    "    #print(df.iloc[-1])\n",
    "    return patterns\n",
    "\n",
    "def get_signals(df, threshold = 0.05):\n",
    "    overlap_indicators = ['ema20', 'ema10', 'sma200',\n",
    "                  'w_ema20', 'w_ema10', 'w_sma200',]\n",
    "    signals = {'proximity_short': [],\n",
    "               'proximity_long': [],\n",
    "               'price_crossover_short': [],\n",
    "               'price_crossover_long': [],}\n",
    "    for indicator in overlap_indicators:\n",
    "        if indicator in df.columns:\n",
    "            #If price is near the indicator (5%), flag the indicator\n",
    "            if (abs(df.iloc[-1]['close'] - df.iloc[-1][indicator])/df.iloc[-1]['close'])<= threshold:\n",
    "                if df.iloc[-1]['close'] < df.iloc[-1][indicator]:\n",
    "                    signals['proximity_short'].append(indicator)\n",
    "                if df.iloc[-1]['close'] > df.iloc[-1][indicator]:\n",
    "                    signals['proximity_long'].append(indicator)\n",
    "            #If price is crossing over the indicator, then flag too\n",
    "            if df.iloc[-1]['low'] < df.iloc[-1][indicator] and df.iloc[-1]['high'] >= df.iloc[-1][indicator]:\n",
    "                if df.iloc[-1]['close'] > df.iloc[-1]['open']:\n",
    "                    signals['price_crossover_short'].append(indicator)\n",
    "                else:\n",
    "                    signals['price_crossover_long'].append(indicator)\n",
    "                \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a582d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_stuff(stock, prefix=''):\n",
    "    listing = get_stock_listing(stock, duration=30, last_date = datetime.date.today(), \n",
    "                                studies={'daily': ['rsi', 'ema20', 'sma200'],})\n",
    "                                         #'weekly':['rsi', 'ema20', 'ema10'],\n",
    "                                         #'monthly': ['rsi']\n",
    "                                        #})\n",
    "    #print(listing.tail())\n",
    "    if len(listing)==0:\n",
    "        return\n",
    "    #if listing.index[-1].date() != datetime.date.today() - datetime.timedelta(days=1):\n",
    "    dateval = datetime.date.today()\n",
    "    if datetime.date.today().weekday() in [5,6]: #If its saturday (5) or sunday (6)\n",
    "        dateval = dateval - datetime.timedelta(days=abs(4-datetime.date.today()))\n",
    "    if listing.index[-1].date() != dateval:\n",
    "        print('No data for today on stock {}'.format(stock))\n",
    "        return\n",
    "    patterns = get_last_day_patterns(listing)\n",
    "    signals = get_signals(listing)\n",
    "    vol_sigs = get_volume_signals(listing)\n",
    "    if len(patterns['bullish'])>0 or len(patterns['bearish'])>0:\n",
    "        print(f\"{prefix}{stock.sid} \\nBullish: {patterns['bullish']}\\tBearish: {patterns['bearish']}\")\n",
    "    else:\n",
    "        print(f\"{prefix}{stock.sid}: No patterns\")\n",
    "    if len(signals['proximity_short'])>0:\n",
    "        print(f\"Bearish Proximity signals: {signals['proximity_short']}\")\n",
    "    if len(signals['proximity_long'])>0:\n",
    "        print(f\"Bullish Proximity signals: {signals['proximity_long']}\")\n",
    "    if len(signals['price_crossover_short'])>0:\n",
    "        print(f\"Bearish Crossover signals: {signals['price_crossover_short']}\")\n",
    "    if len(signals['price_crossover_long'])>0:\n",
    "        print(f\"Bullish Crossover signals: {signals['price_crossover_long']}\")\n",
    "    for key in vol_sigs:\n",
    "        if vol_sigs[key] is not None:\n",
    "            print(f\"{key}: {vol_sigs[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95144bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#List of FnO stocks (where bullish and bearish signals are required)\n",
    "fno = []\n",
    "with open('fno.txt', 'r') as fd:\n",
    "    for line in fd:\n",
    "        fno.append(line.strip())\n",
    "fno = sorted(fno)\n",
    "\n",
    "for sname in fno:\n",
    "    try:\n",
    "        stock = Stock.objects.get(sid=sname)\n",
    "        do_stuff(stock, prefix='[FnO]')\n",
    "    except Stock.DoesNotExist:\n",
    "        print(f'{sname} name not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15878e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Bearish scans for these too\n",
    "portfolio = []\n",
    "with open('portfolio.txt', 'r') as fd:\n",
    "    for line in fd:\n",
    "        portfolio.append(line.strip())\n",
    "portfolio = sorted(portfolio)\n",
    "for sname in portfolio:\n",
    "    try:\n",
    "        stock = Stock.objects.get(sid=sname)\n",
    "        do_stuff(stock, prefix='[PF]')\n",
    "    except Stock.DoesNotExist:\n",
    "        print(f'{sname} name not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ac1b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "margin_stocks = []\n",
    "with open('margin.txt', 'r') as fd:\n",
    "    for line in fd:\n",
    "        margin_stocks.append(line.strip())\n",
    "        \n",
    "for stock in Stock.objects.all():\n",
    "    #listing = get_stock_listing(stock, duration=30, last_date = datetime.date(2020, 12, 31))\n",
    "    #print(stock)\n",
    "    if (stock.sid in fno) or (stock.sid in portfolio):\n",
    "        continue\n",
    "    if stock.sid in margin_stocks:\n",
    "        do_stuff(stock, prefix='[MG]')\n",
    "    else:\n",
    "        do_stuff(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea84d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock = Stock.objects.get(sid='RAIN')\n",
    "\n",
    "print (stock.sid)\n",
    "print(stock.security)\n",
    "print (stock)\n",
    "listing = get_stock_listing(stock, duration=300, last_date = datetime.date.today())\n",
    "listing.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3778d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from indicators import *\n",
    "tolerance = 1 #days\n",
    "df  = listing.rename(columns={\"open\": \"Open\", \"high\": \"High\", \"low\": \"Low\", \"close\":\"Close\", \"volume\":\"Volume\"})\n",
    "divergences = autodetect_divergence(ohlc = df, indicator_data=talib.RSI(df['Close'], 14), tolerance = tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46807849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.iloc[divergences.index[divergences['regularBull']>0] | divergences.index[divergences['hiddenBull']>0]]\n",
    "df.iloc[divergences.index[divergences['regularBull']>0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb183c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signals = get_volume_signals(listing)\n",
    "print(signals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
